[2024-02-22 18:28:25,910][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-22 18:28:25,911][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-22 18:28:25,913][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-22 18:28:25,913][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-22 18:28:25,914][__main__][INFO] - Loading task
[2024-02-22 18:28:29,651][__main__][INFO] - Loading model
[2024-02-22 18:28:45,474][__main__][INFO] - Loading criterion
[2024-02-22 18:28:45,475][__main__][INFO] - Loading optimizer
[2024-02-22 18:28:45,478][__main__][INFO] - Loading lr scheduler
[2024-02-22 18:28:45,480][__main__][INFO] - Loading dataloaders
[2024-02-22 18:28:45,480][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:28:21,169][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-22 20:28:21,171][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-22 20:28:21,171][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-22 20:28:21,172][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-22 20:28:21,172][__main__][INFO] - Loading task
[2024-02-22 20:28:21,473][__main__][INFO] - Loading model
[2024-02-22 20:28:23,235][__main__][INFO] - Loading criterion
[2024-02-22 20:28:23,235][__main__][INFO] - Loading optimizer
[2024-02-22 20:28:23,239][__main__][INFO] - Loading lr scheduler
[2024-02-22 20:28:23,240][__main__][INFO] - Loading dataloaders
[2024-02-22 20:28:23,240][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:28:24,152][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:28:24,152][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:28:25,965][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-22 20:28:26,835][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:28:26,836][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-22 20:28:26,913][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-22 20:28:26,913][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:28:28,157][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-22 20:28:29,054][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:28:29,054][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-22 20:28:29,089][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-22 20:28:29,089][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:28:30,301][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-22 20:28:33,473][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-22 20:28:33,511][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-22 20:28:36,685][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-22 20:28:36,809][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 23.328125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 0.001953125, 'gpu_5_mem_used_gb': 0.001953125, 'gpu_6_mem_used_gb': 0.001953125, 'gpu_7_mem_used_gb': 0.001953125}
[2024-02-22 20:28:36,809][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-22 20:28:36,809][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-22 20:28:36,809][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 600
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 600
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 600
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-22 20:28:36,810][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-22 20:35:07,253][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-22 20:35:07,254][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-22 20:35:07,255][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-22 20:35:07,256][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-22 20:35:07,256][__main__][INFO] - Loading task
[2024-02-22 20:35:07,586][__main__][INFO] - Loading model
[2024-02-22 20:35:09,325][__main__][INFO] - Loading criterion
[2024-02-22 20:35:09,325][__main__][INFO] - Loading optimizer
[2024-02-22 20:35:09,329][__main__][INFO] - Loading lr scheduler
[2024-02-22 20:35:09,330][__main__][INFO] - Loading dataloaders
[2024-02-22 20:35:09,330][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:35:10,218][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:35:10,218][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:35:11,735][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-22 20:35:12,485][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:35:12,486][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-22 20:35:12,531][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-22 20:35:12,531][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:35:14,074][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-22 20:35:14,908][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:35:14,908][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-22 20:35:14,942][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-22 20:35:14,942][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:35:16,149][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-22 20:35:19,210][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-22 20:35:19,248][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-22 20:35:21,920][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 23.060546875, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 0.001953125, 'gpu_5_mem_used_gb': 0.001953125, 'gpu_6_mem_used_gb': 0.001953125, 'gpu_7_mem_used_gb': 0.001953125}
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-22 20:35:22,049][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 600
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 600
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 600
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-22 20:35:22,050][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-22 20:36:31,364][__main__][INFO] - task: CLIPTask
[2024-02-22 20:36:31,364][__main__][INFO] - model: DeepSpeedEngine
[2024-02-22 20:36:31,366][__main__][INFO] - num. model params: 986M
[2024-02-22 20:36:31,369][__main__][INFO] - num. model trainable params: 986M
[2024-02-22 20:36:31,369][__main__][INFO] - criterion: CLIPCriterion
[2024-02-22 20:36:31,369][__main__][INFO] - num. train examples: 10
[2024-02-22 20:36:31,369][__main__][INFO] - num. valid examples: 100
[2024-02-22 20:36:31,369][__main__][INFO] - num. test examples: 100
[2024-02-22 20:38:22,450][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-22 20:38:22,450][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-22 20:38:22,450][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-22 20:38:22,451][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-22 20:38:22,451][__main__][INFO] - Loading task
[2024-02-22 20:38:22,733][__main__][INFO] - Loading model
[2024-02-22 20:38:36,936][__main__][INFO] - Loading criterion
[2024-02-22 20:38:36,937][__main__][INFO] - Loading optimizer
[2024-02-22 20:38:36,940][__main__][INFO] - Loading lr scheduler
[2024-02-22 20:38:36,942][__main__][INFO] - Loading dataloaders
[2024-02-22 20:38:36,942][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:38:37,761][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:38:37,762][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:38:39,346][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-22 20:38:40,237][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:38:40,237][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-22 20:38:40,297][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-22 20:38:40,297][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:38:41,501][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-22 20:38:42,341][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:38:42,341][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-22 20:38:42,375][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-22 20:38:42,375][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:38:43,576][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-22 20:38:46,668][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-22 20:38:46,706][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-22 20:38:49,285][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-22 20:38:49,409][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 23.060546875, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 0.001953125, 'gpu_5_mem_used_gb': 0.001953125, 'gpu_6_mem_used_gb': 0.001953125, 'gpu_7_mem_used_gb': 0.001953125}
[2024-02-22 20:38:49,409][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-22 20:38:49,409][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-22 20:38:49,409][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 600
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 600
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 600
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-22 20:38:49,410][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-22 20:38:57,079][__main__][INFO] - task: CLIPTask
[2024-02-22 20:38:57,079][__main__][INFO] - model: DeepSpeedEngine
[2024-02-22 20:38:57,081][__main__][INFO] - num. model params: 986M
[2024-02-22 20:38:57,083][__main__][INFO] - num. model trainable params: 986M
[2024-02-22 20:38:57,083][__main__][INFO] - criterion: CLIPCriterion
[2024-02-22 20:38:57,084][__main__][INFO] - num. train examples: 10
[2024-02-22 20:38:57,084][__main__][INFO] - num. valid examples: 100
[2024-02-22 20:38:57,084][__main__][INFO] - num. test examples: 100
[2024-02-22 20:39:06,685][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:39:07,620][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:07,620][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:09,660][__main__][INFO] - *** Evaluating validation ***
[2024-02-22 20:39:09,660][trainer.tasks.clip_task][INFO] - Running clip score...
[2024-02-22 20:39:12,142][trainer.tasks.base_task][INFO] - Gathering dict from all processes...
[2024-02-22 20:39:12,310][trainer.tasks.base_task][INFO] - Uploading to wandb
[2024-02-22 20:39:14,988][trainer.accelerators.base_accelerator][INFO] - Metrics: {'accuracy': 0.33, 'num_samples': 100}
[2024-02-22 20:39:16,041][trainer.accelerators.base_accelerator][INFO] - Epoch 0 finished
[2024-02-22 20:39:24,031][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:39:24,858][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:24,858][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:27,220][trainer.accelerators.base_accelerator][INFO] - Epoch 1 finished
[2024-02-22 20:39:34,002][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:39:34,857][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:34,857][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:37,345][trainer.accelerators.base_accelerator][INFO] - Epoch 2 finished
[2024-02-22 20:39:44,205][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:39:45,011][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:45,011][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:47,337][trainer.accelerators.base_accelerator][INFO] - Epoch 3 finished
[2024-02-22 20:39:54,291][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:39:55,091][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:55,091][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:39:57,476][trainer.accelerators.base_accelerator][INFO] - Epoch 4 finished
[2024-02-22 20:40:04,396][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:05,232][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:05,232][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:07,592][trainer.accelerators.base_accelerator][INFO] - Epoch 5 finished
[2024-02-22 20:40:14,500][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:15,301][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:15,302][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:17,665][trainer.accelerators.base_accelerator][INFO] - Epoch 6 finished
[2024-02-22 20:40:24,806][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:25,605][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:25,605][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:27,960][trainer.accelerators.base_accelerator][INFO] - Epoch 7 finished
[2024-02-22 20:40:35,216][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:36,089][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:36,089][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:38,464][trainer.accelerators.base_accelerator][INFO] - Epoch 8 finished
[2024-02-22 20:40:45,323][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:46,246][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:46,246][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:48,678][trainer.accelerators.base_accelerator][INFO] - Epoch 9 finished
[2024-02-22 20:40:55,655][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:40:56,467][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:56,467][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:40:58,983][trainer.accelerators.base_accelerator][INFO] - Epoch 10 finished
[2024-02-22 20:52:14,052][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-22 20:52:14,053][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-22 20:52:14,054][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-22 20:52:14,054][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-22 20:52:14,054][__main__][INFO] - Loading task
[2024-02-22 20:52:14,365][__main__][INFO] - Loading model
[2024-02-22 20:52:31,638][__main__][INFO] - Loading criterion
[2024-02-22 20:52:31,639][__main__][INFO] - Loading optimizer
[2024-02-22 20:52:31,642][__main__][INFO] - Loading lr scheduler
[2024-02-22 20:52:31,643][__main__][INFO] - Loading dataloaders
[2024-02-22 20:52:31,644][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:52:32,920][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:52:32,920][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:52:34,581][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-22 20:52:35,413][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:52:35,413][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-22 20:52:35,460][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-22 20:52:35,460][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-22 20:52:36,800][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-22 20:52:37,683][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:52:37,683][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-22 20:52:37,740][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-22 20:52:37,740][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-22 20:52:39,082][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-22 20:52:42,158][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-22 20:52:42,195][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-22 20:52:44,786][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-22 20:52:44,919][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 23.060546875, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 0.001953125, 'gpu_5_mem_used_gb': 0.001953125, 'gpu_6_mem_used_gb': 0.001953125, 'gpu_7_mem_used_gb': 0.001953125}
[2024-02-22 20:52:44,919][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-22 20:52:44,919][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-22 20:52:44,919][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-22 20:52:44,919][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 600
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 600
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 600
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-22 20:52:44,920][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-22 20:52:52,516][__main__][INFO] - task: CLIPTask
[2024-02-22 20:52:52,516][__main__][INFO] - model: DeepSpeedEngine
[2024-02-22 20:52:52,518][__main__][INFO] - num. model params: 986M
[2024-02-22 20:52:52,520][__main__][INFO] - num. model trainable params: 986M
[2024-02-22 20:52:52,520][__main__][INFO] - criterion: CLIPCriterion
[2024-02-22 20:52:52,521][__main__][INFO] - num. train examples: 10
[2024-02-22 20:52:52,521][__main__][INFO] - num. valid examples: 100
[2024-02-22 20:52:52,521][__main__][INFO] - num. test examples: 100
[2024-02-22 20:53:01,751][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-22 20:53:02,563][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:53:02,564][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-22 20:53:04,538][__main__][INFO] - *** Evaluating validation ***
[2024-02-22 20:53:04,539][trainer.tasks.clip_task][INFO] - Running clip score...
[2024-02-22 20:53:07,068][trainer.tasks.base_task][INFO] - Gathering dict from all processes...
[2024-02-22 20:53:07,268][trainer.tasks.base_task][INFO] - Uploading to wandb
[2024-02-22 20:53:09,991][trainer.accelerators.base_accelerator][INFO] - Metrics: {'accuracy': 0.33, 'num_samples': 100}
[2024-02-22 20:53:10,654][trainer.accelerators.base_accelerator][INFO] - Epoch 0 finished
[2024-02-23 15:04:34,028][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 15:04:34,029][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 15:04:34,029][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 15:04:34,029][__main__][INFO] - Loading task
[2024-02-23 15:04:34,353][__main__][INFO] - Loading model
[2024-02-23 15:04:35,426][__main__][INFO] - Loading criterion
[2024-02-23 15:04:35,427][__main__][INFO] - Loading optimizer
[2024-02-23 15:04:35,430][__main__][INFO] - Loading lr scheduler
[2024-02-23 15:04:35,432][__main__][INFO] - Loading dataloaders
[2024-02-23 15:04:35,432][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 15:04:36,192][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 15:04:36,192][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 15:04:37,490][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 15:04:38,242][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 15:04:38,242][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 15:04:38,272][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 15:04:38,272][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 15:04:39,538][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 15:04:40,296][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 15:04:40,296][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 15:04:40,326][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 15:04:40,326][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 15:04:41,760][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 15:04:45,384][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 15:04:45,417][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 15:04:48,758][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 15:04:49,062][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 15:04:49,062][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 600
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 600
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 600
[2024-02-23 15:04:49,063][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 15:04:49,064][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 15:04:52,172][__main__][INFO] - task: CLIPTask
[2024-02-23 15:04:52,172][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 15:04:52,175][__main__][INFO] - num. model params: 986M
[2024-02-23 15:04:52,176][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 15:04:52,176][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 15:04:52,177][__main__][INFO] - num. train examples: 10
[2024-02-23 15:04:52,177][__main__][INFO] - num. valid examples: 100
[2024-02-23 15:04:52,177][__main__][INFO] - num. test examples: 100
[2024-02-23 16:14:24,805][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:14:24,805][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:14:24,805][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:14:24,805][__main__][INFO] - Loading task
[2024-02-23 16:14:24,897][__main__][INFO] - Loading model
[2024-02-23 16:14:25,947][__main__][INFO] - Loading criterion
[2024-02-23 16:14:25,948][__main__][INFO] - Loading optimizer
[2024-02-23 16:14:25,951][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:14:25,952][__main__][INFO] - Loading dataloaders
[2024-02-23 16:14:25,953][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:15:45,701][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:15:45,701][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:15:45,701][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:15:45,701][__main__][INFO] - Loading task
[2024-02-23 16:15:45,980][__main__][INFO] - Loading model
[2024-02-23 16:15:47,163][__main__][INFO] - Loading criterion
[2024-02-23 16:15:47,164][__main__][INFO] - Loading optimizer
[2024-02-23 16:15:47,167][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:15:47,169][__main__][INFO] - Loading dataloaders
[2024-02-23 16:15:47,169][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:15:47,911][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:15:47,911][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:15:49,577][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 16:15:50,309][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:15:50,309][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 16:15:50,342][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 16:15:50,343][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:15:51,583][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 16:15:52,303][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:15:52,303][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 16:15:52,331][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 16:15:52,331][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:15:53,552][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 16:15:56,367][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 16:15:56,401][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 16:16:03,150][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 16:16:03,477][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 16:16:03,477][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 500
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 500
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 16:16:03,478][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 16:16:03,479][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 500
[2024-02-23 16:16:03,479][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 16:16:03,479][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 16:16:07,492][__main__][INFO] - task: CLIPTask
[2024-02-23 16:16:07,492][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 16:16:07,494][__main__][INFO] - num. model params: 986M
[2024-02-23 16:16:07,495][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 16:16:07,495][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 16:16:07,495][__main__][INFO] - num. train examples: 10
[2024-02-23 16:16:07,496][__main__][INFO] - num. valid examples: 100
[2024-02-23 16:16:07,496][__main__][INFO] - num. test examples: 100
[2024-02-23 16:18:44,005][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:18:44,006][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:18:44,006][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:18:44,006][__main__][INFO] - Loading task
[2024-02-23 16:18:44,281][__main__][INFO] - Loading model
[2024-02-23 16:18:45,346][__main__][INFO] - Loading criterion
[2024-02-23 16:18:45,347][__main__][INFO] - Loading optimizer
[2024-02-23 16:18:45,350][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:18:45,351][__main__][INFO] - Loading dataloaders
[2024-02-23 16:18:45,352][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:18:46,089][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:18:46,089][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:18:47,314][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 16:18:48,028][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:18:48,028][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 16:18:48,059][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 16:18:48,059][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:18:49,681][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 16:18:50,405][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:18:50,405][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 16:18:50,434][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 16:18:50,434][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:18:51,948][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 16:18:55,436][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 16:18:55,471][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 16:18:58,847][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 16:18:59,114][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 16:18:59,114][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 16:18:59,114][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 16:18:59,114][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 500
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 500
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 500
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 16:18:59,115][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 16:19:02,202][__main__][INFO] - task: CLIPTask
[2024-02-23 16:19:02,202][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 16:19:02,205][__main__][INFO] - num. model params: 986M
[2024-02-23 16:19:02,206][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 16:19:02,206][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 16:19:02,207][__main__][INFO] - num. train examples: 10
[2024-02-23 16:19:02,207][__main__][INFO] - num. valid examples: 100
[2024-02-23 16:19:02,207][__main__][INFO] - num. test examples: 100
[2024-02-23 16:20:05,992][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:20:05,992][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:20:05,993][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:20:05,993][__main__][INFO] - Loading task
[2024-02-23 16:20:06,094][__main__][INFO] - Loading model
[2024-02-23 16:20:06,942][__main__][INFO] - Loading criterion
[2024-02-23 16:20:06,943][__main__][INFO] - Loading optimizer
[2024-02-23 16:20:06,945][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:20:06,947][__main__][INFO] - Loading dataloaders
[2024-02-23 16:20:06,947][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:20:07,797][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:20:07,797][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:20:09,123][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 16:20:09,849][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:20:09,850][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 16:20:09,880][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 16:20:09,881][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:20:11,250][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 16:20:11,961][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:20:11,961][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 16:20:11,989][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 16:20:11,989][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:20:13,223][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 16:20:15,814][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 16:20:15,848][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 16:20:19,191][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 16:20:19,305][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 16:20:19,305][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 16:20:19,305][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 16:20:19,305][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 16:20:19,305][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 500
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 500
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 500
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 16:20:19,306][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 16:20:23,026][__main__][INFO] - task: CLIPTask
[2024-02-23 16:20:23,026][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 16:20:23,028][__main__][INFO] - num. model params: 986M
[2024-02-23 16:20:23,030][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 16:20:23,030][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 16:20:23,030][__main__][INFO] - num. train examples: 10
[2024-02-23 16:20:23,030][__main__][INFO] - num. valid examples: 100
[2024-02-23 16:20:23,030][__main__][INFO] - num. test examples: 100
[2024-02-23 16:25:22,409][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:25:22,409][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:25:22,410][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:25:22,410][__main__][INFO] - Loading task
[2024-02-23 16:25:22,706][__main__][INFO] - Loading model
[2024-02-23 16:25:23,799][__main__][INFO] - Loading criterion
[2024-02-23 16:25:23,799][__main__][INFO] - Loading optimizer
[2024-02-23 16:25:23,802][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:25:23,804][__main__][INFO] - Loading dataloaders
[2024-02-23 16:25:23,804][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:25:24,544][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:25:24,544][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:25:25,923][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 16:25:26,637][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:25:26,637][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 16:25:26,668][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 16:25:26,668][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:25:28,067][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 16:25:28,783][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:25:28,783][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 16:25:28,812][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 16:25:28,812][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:25:30,089][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 16:25:32,975][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 16:25:33,009][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 16:25:36,388][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 16:25:36,535][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 16:25:36,536][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 500
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 500
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 500
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 16:25:36,537][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 16:25:40,322][__main__][INFO] - task: CLIPTask
[2024-02-23 16:25:40,323][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 16:25:40,325][__main__][INFO] - num. model params: 986M
[2024-02-23 16:25:40,326][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 16:25:40,327][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 16:25:40,327][__main__][INFO] - num. train examples: 10
[2024-02-23 16:25:40,327][__main__][INFO] - num. valid examples: 100
[2024-02-23 16:25:40,327][__main__][INFO] - num. test examples: 100
[2024-02-23 16:26:04,321][trainer.accelerators.base_accelerator][INFO] - Setting seed 42
[2024-02-23 16:26:04,321][trainer.accelerators.base_accelerator][INFO] - Initialized accelerator: rank=0
[2024-02-23 16:26:04,322][__main__][INFO] - Config can be found in test_output/config.yaml
[2024-02-23 16:26:04,322][__main__][INFO] - Loading task
[2024-02-23 16:26:04,618][__main__][INFO] - Loading model
[2024-02-23 16:26:05,642][__main__][INFO] - Loading criterion
[2024-02-23 16:26:05,642][__main__][INFO] - Loading optimizer
[2024-02-23 16:26:05,645][__main__][INFO] - Loading lr scheduler
[2024-02-23 16:26:05,647][__main__][INFO] - Loading dataloaders
[2024-02-23 16:26:05,647][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:26:06,386][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:26:06,386][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:26:07,682][trainer.datasetss.my_hf_dataset][INFO] - Loading validation dataset
[2024-02-23 16:26:08,435][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:26:08,435][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in validation split
[2024-02-23 16:26:08,466][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from validation dataset
[2024-02-23 16:26:08,466][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from validation dataset
[2024-02-23 16:26:09,791][trainer.datasetss.my_hf_dataset][INFO] - Loading test dataset
[2024-02-23 16:26:10,514][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:26:10,514][trainer.datasetss.my_hf_dataset][INFO] - Keeping only examples with label in test split
[2024-02-23 16:26:10,543][trainer.datasetss.my_hf_dataset][INFO] - Kept 100 examples from test dataset
[2024-02-23 16:26:10,543][trainer.datasetss.my_hf_dataset][INFO] - Loaded 100 examples from test dataset
[2024-02-23 16:26:11,801][accelerate.accelerator][INFO] - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (16).
[2024-02-23 16:26:14,531][trainer.accelerators.base_accelerator][INFO] - No checkpoint found, training from scratch
[2024-02-23 16:26:14,566][trainer.accelerators.base_accelerator][INFO] - Initializing trackers
[2024-02-23 16:26:17,917][trainer.accelerators.base_accelerator][INFO] - Training config:
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] - nvidia-smi stats: {'gpu_0_mem_used_gb': 29.26171875, 'gpu_1_mem_used_gb': 7.052734375, 'gpu_2_mem_used_gb': 51.181640625, 'gpu_3_mem_used_gb': 4.72265625}
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] - ***** Running training *****
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] -   Instantaneous batch size per device = 16
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] -   Total train batch size (w. parallel, distributed & accumulation) = 16
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] -   Gradient Accumulation steps = 1
[2024-02-23 16:26:18,200][trainer.accelerators.base_accelerator][INFO] -   Total warmup steps = 500
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Total training steps = 500
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Total epochs = 500
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Steps per epoch = 1
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Update steps per epoch = 1
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Total optimization steps = 500
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   Mixed precision = bf16
[2024-02-23 16:26:18,201][trainer.accelerators.base_accelerator][INFO] -   World size = 1
[2024-02-23 16:26:21,158][__main__][INFO] - task: CLIPTask
[2024-02-23 16:26:21,158][__main__][INFO] - model: DeepSpeedEngine
[2024-02-23 16:26:21,160][__main__][INFO] - num. model params: 986M
[2024-02-23 16:26:21,162][__main__][INFO] - num. model trainable params: 986M
[2024-02-23 16:26:21,162][__main__][INFO] - criterion: CLIPCriterion
[2024-02-23 16:26:21,162][__main__][INFO] - num. train examples: 10
[2024-02-23 16:26:21,162][__main__][INFO] - num. valid examples: 100
[2024-02-23 16:26:21,162][__main__][INFO] - num. test examples: 100
[2024-02-23 16:26:28,808][trainer.datasetss.my_hf_dataset][INFO] - Loading train dataset
[2024-02-23 16:26:29,540][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:26:29,540][trainer.datasetss.my_hf_dataset][INFO] - Loaded 10 examples from train dataset
[2024-02-23 16:26:31,616][__main__][INFO] - *** Evaluating validation ***
[2024-02-23 16:26:31,616][trainer.tasks.clip_task][INFO] - Running clip score...
[2024-02-23 16:26:34,465][trainer.tasks.base_task][INFO] - Gathering dict from all processes...
[2024-02-23 16:26:34,641][trainer.tasks.base_task][INFO] - Uploading to wandb
[2024-02-23 16:26:36,852][trainer.accelerators.base_accelerator][INFO] - Metrics: {'accuracy': 0.33, 'num_samples': 100}
[2024-02-23 16:26:37,348][trainer.accelerators.base_accelerator][INFO] - Epoch 0 finished
[2024-02-23 16:26:38,391][trainer.accelerators.base_accelerator][INFO] - Epoch 1 finished
[2024-02-23 16:26:39,224][trainer.accelerators.base_accelerator][INFO] - Epoch 2 finished
[2024-02-23 16:26:40,064][trainer.accelerators.base_accelerator][INFO] - Epoch 3 finished
[2024-02-23 16:26:40,885][trainer.accelerators.base_accelerator][INFO] - Epoch 4 finished
[2024-02-23 16:26:41,698][trainer.accelerators.base_accelerator][INFO] - Epoch 5 finished
[2024-02-23 16:26:42,556][trainer.accelerators.base_accelerator][INFO] - Epoch 6 finished
[2024-02-23 16:26:43,383][trainer.accelerators.base_accelerator][INFO] - Epoch 7 finished
[2024-02-23 16:26:44,214][trainer.accelerators.base_accelerator][INFO] - Epoch 8 finished
[2024-02-23 16:26:45,028][trainer.accelerators.base_accelerator][INFO] - Epoch 9 finished
[2024-02-23 16:26:45,856][trainer.accelerators.base_accelerator][INFO] - Epoch 10 finished
[2024-02-23 16:26:46,668][trainer.accelerators.base_accelerator][INFO] - Epoch 11 finished
[2024-02-23 16:26:47,455][trainer.accelerators.base_accelerator][INFO] - Epoch 12 finished
[2024-02-23 16:26:48,292][trainer.accelerators.base_accelerator][INFO] - Epoch 13 finished
[2024-02-23 16:26:49,120][trainer.accelerators.base_accelerator][INFO] - Epoch 14 finished
[2024-02-23 16:26:50,568][trainer.accelerators.base_accelerator][INFO] - Epoch 15 finished
[2024-02-23 16:26:51,396][trainer.accelerators.base_accelerator][INFO] - Epoch 16 finished
[2024-02-23 16:26:52,200][trainer.accelerators.base_accelerator][INFO] - Epoch 17 finished
[2024-02-23 16:26:52,953][trainer.accelerators.base_accelerator][INFO] - Epoch 18 finished
[2024-02-23 16:26:53,765][trainer.accelerators.base_accelerator][INFO] - Epoch 19 finished
[2024-02-23 16:26:54,601][trainer.accelerators.base_accelerator][INFO] - Epoch 20 finished
